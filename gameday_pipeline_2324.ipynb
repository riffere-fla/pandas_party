{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from prefect.blocks.system import Secret\n",
    "from catnip.fla_redshift import FLA_Redshift\n",
    "from typing import Dict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from fractions import Fraction\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redshift_credentials() -> Dict:\n",
    "\n",
    "    cred_dict = {\n",
    "        \"dbname\": Secret.load(\"stellar-redshift-db-name\").get(),\n",
    "        \"host\": Secret.load(\"stellar-redshift-host\").get(),\n",
    "        \"port\": 5439,\n",
    "        \"user\": Secret.load(\"stellar-redshift-user-name\").get(),\n",
    "        \"password\": Secret.load(\"stellar-redshift-password\").get(),\n",
    "\n",
    "        \"aws_access_key_id\": Secret.load(\"fla-s3-aws-access-key-id-east-1\").get(),\n",
    "        \"aws_secret_access_key\": Secret.load(\"fla-s3-aws-secret-access-key-east-1\").get(),\n",
    "        \"bucket\": Secret.load(\"fla-s3-bucket-name-east-1\").get(),\n",
    "        \"subdirectory\": \"us-east-1\",\n",
    "\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    return cred_dict\n",
    "\n",
    "with ThreadPoolExecutor(1) as pool:\n",
    "    rs_creds = pool.submit(lambda: get_redshift_credentials()).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pricing - Dynamic Pricing Model\n",
    "\n",
    "# seperate file (pricing_model_v1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Predicted Budget\n",
    "\n",
    "# total budget goal divided by # of games in tier & ticket type %s (can add dow to break tiers down also?)\n",
    "\n",
    "games = pd.read_csv(\"C:\\\\Users\\\\riffere\\\\OneDrive - Florida Panthers\\\\Documents\\\\final_schedule_2324.csv\") # can switch to warehouse call when games get put in!!!!!!!!!!!!!!!!!\n",
    "games['key'] = 0\n",
    "\n",
    "ticket_types = pd.DataFrame(\n",
    "    {'ticket_type' : ['Annual Suites','Full','Premier', 'Flex & Quarter', 'Groups', 'Nightly Suites',\n",
    "    'Secondary', 'Singles', 'Sponsor', 'Trade'], \n",
    "    'ticket_percent' : [Fraction(5250000, 50500000), Fraction(21700000, 50500000), Fraction(7000000, 50500000),\n",
    "    Fraction(1000000, 50500000), Fraction(3250000, 50500000), Fraction(2250000, 50500000), Fraction(5050000, 50500000),\n",
    "    Fraction(4950000, 50500000), Fraction(1350000, 50500000), Fraction(700000, 50500000)]})\n",
    "ticket_types['key'] = 0\n",
    "\n",
    "df = games.merge(ticket_types, how = 'outer', on = 'key')\n",
    "\n",
    "tier_percentage = pd.DataFrame({'Final' : ['AP','BP','CP','C','DP','D','E'],\n",
    "                                'Full' : [0.075, 0.175, 0.3, 0.05, 0.115, 0.115, 0.17], 'Premier': [0.10, 0.27, 0.46, 0, 0.17, 0, 0],\n",
    "                                'Annual Suites' : [0.08, 0.16, 0.291, 0.049, 0.115, 0.115, 0.19], 'Flex & Quarter' : [0.092, 0.22, 0.294, 0.064, 0.107, 0.08, 0.143],\n",
    "                                'Groups' : [0.10, 0.127, 0.255, 0.058, 0.10, 0.15, 0.21], 'Nightly Suites' : [0.088, 0.2, 0.284, 0.067, 0.122, 0.1, 0.139],\n",
    "                                'Secondary' : [0.09, 0.19, 0.325, 0.055, 0.12, 0.12, 0.10], 'Singles' : [0.083, 0.182, 0.264, 0.063, 0.084, 0.135, 0.189],\n",
    "                                'Sponsor' : [0.076, 0.181, 0.31, 0.051, 0.119, 0.114, 0.149], 'Trade' : [0.073, 0.161, 0.3, 0.053, 0.127, 0.116, 0.17]})\n",
    "\n",
    "\n",
    "melt_tiers = tier_percentage.melt(id_vars = ['Final'], value_vars = ['Full', 'Annual Suites', 'Flex & Quarter', 'Groups', 'Premier',\n",
    "                                                                    'Nightly Suites', 'Secondary', 'Singles', 'Sponsor', 'Trade'],\n",
    "                                    var_name = 'ticket_type', value_name = 'tier_percent')\n",
    "\n",
    "num_games = pd.DataFrame({'Final' : ['AP', 'BP', 'CP', 'C', 'DP', 'D', 'E'], 'num_games' : [2, 6, 12, 2, 5, 5, 9]})\n",
    "\n",
    "df = df.merge(melt_tiers, how = 'left', on = ['Final','ticket_type'])\n",
    "df = df.merge(num_games, how = 'left', on = 'Final')\n",
    "\n",
    "df['goal'] = 52500000\n",
    "df['small_total'] = [(w/x)*y*z for w,x,y,z in zip(df['goal'], df['num_games'], df['tier_percent'], df['ticket_percent'])]\n",
    "df = df[['EventDate','Away','Final','ticket_type', 'small_total']]\n",
    "    \n",
    "totals = df.groupby(by = 'EventDate').sum()['small_total']\n",
    "totals = totals.to_frame()\n",
    "totals.reset_index(inplace = True)\n",
    "df = df.merge(right = totals, how = 'left', on = 'EventDate')\n",
    "\n",
    "df['percent'] = [x/y for x,y in zip(df['small_total_x'], df['small_total_y'])]\n",
    "new_totals = pd.DataFrame(\n",
    "    {'Final' :['AP', 'BP', 'CP', 'C', 'DP', 'D', 'E'], 'budget' : [2290775, 1712950, 1445504, 1220525, 1203420, 1030220, 742828]})\n",
    "df = df.merge(right = new_totals, how = 'left', on = 'Final')\n",
    "df['new_small_total'] = [x*y for x,y in zip(df['percent'], df['budget'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Predicted Revenue & Tickets\n",
    "\n",
    "# classification model for tickets then use avg (loc, ticket type, tier) atp for revenue\n",
    "\n",
    "# revenue pacing based on current ATP and days out based on historical numbers\n",
    "# q = \"\"\"\n",
    "# with a as\n",
    "#     (select \n",
    "#         cth_ticket_expanded_all.event_date, tier, cast(date_diff('day', cast(cth_ticket_expanded_all.event_date as date), getdate()) as int) as days_out, sum(block_purchase_price) as gross_rev,\n",
    "#         sum(paid_seats) as paid_seats,\n",
    "#         CASE\n",
    "#             when sum(paid_seats) = 0 then 0\n",
    "#             else gross_rev/sum(paid_seats)\n",
    "#         end as avg_atp\n",
    "#     from \n",
    "#         custom.cth_ticket_expanded_all\n",
    "#     left join \n",
    "#         custom.cth_game_descriptions \n",
    "#         on cth_ticket_expanded_all.event_date = cth_game_descriptions.event_date\n",
    "#     group by \n",
    "#         cth_ticket_expanded_all.event_date, tier)\n",
    "\n",
    "# select \n",
    "#     *\n",
    "# from \n",
    "#     a\n",
    "# left join \n",
    "#     custom.cth_expected_singles_012723\n",
    "#     on a.tier = cth_expected_singles_012723.tier\n",
    "#     and a.days_out = cth_expected_singles_012723.days_out\n",
    "# order by \n",
    "#     event_date\n",
    "# \"\"\"\n",
    "\n",
    "# df = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Predicted Attendance\n",
    "\n",
    "# mulitply projected tickets by historical show rate\n",
    "\n",
    "q = \"\"\"\n",
    "WITH a as\n",
    "    (SELECT \n",
    "        h.paid_seats, h.did_attend, h.event_date,\n",
    "        CASE\n",
    "        WHEN h.pc_one IN ('A','B','C','D','E','F','G','H','I','J','1','2','3','4','5','6','7','8') THEN 'Lowers'\n",
    "        WHEN h.pc_one IN ('K','L','M') THEN 'Clubs'\n",
    "        WHEN h.pc_one IN ('N','O','P','Q','R','S','T') THEN 'Uppers'\n",
    "        WHEN h.pc_one IN ('U','V','W') THEN 'Suites'\n",
    "        WHEN h.pc_one IN ('X') THEN 'Lounge 954'\n",
    "        WHEN h.pc_one IN ('Y') THEN 'Loft'\n",
    "        WHEN h.pc_one IN ('Z') THEN 'Corona'\n",
    "        END as \"location\"\n",
    "    FROM \n",
    "        custom.cth_historical_all_1718_2223 h\n",
    "    WHERE \n",
    "        h.season in ('2019-20', '2021-22', '2022-23')\n",
    "        and h.paid_seats != 0)\n",
    "\n",
    "SELECT \n",
    "    g.tier, a.location, sum(a.paid_seats)/count(distinct(a.event_date)) as paid_seats, sum(a.did_attend)/count(distinct(a.event_date)) as did_attend, \n",
    "    sum(a.did_attend)/sum(a.paid_seats)\n",
    "FROM \n",
    "    a\n",
    "LEFT JOIN \n",
    "    custom.cth_game_descriptions g on a.event_date = g.event_date\n",
    "GROUP BY \n",
    "    tier, location\n",
    "ORDER BY \n",
    "    tier, location\"\"\"\n",
    "\n",
    "paid_seats = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)\n",
    "\n",
    "q = \"\"\"\n",
    "WITH a as\n",
    "    (SELECT \n",
    "        h.comp_seats, h.did_attend, h.event_date,\n",
    "        CASE\n",
    "        WHEN h.pc_one IN ('A','B','C','D','E','F','G','H','I','J','1','2','3','4','5','6','7','8') THEN 'Lowers'\n",
    "        WHEN h.pc_one IN ('K','L','M') THEN 'Clubs'\n",
    "        WHEN h.pc_one IN ('N','O','P','Q','R','S','T') THEN 'Uppers'\n",
    "        WHEN h.pc_one IN ('U','V','W') THEN 'Suites'\n",
    "        WHEN h.pc_one IN ('X') THEN 'Lounge 954'\n",
    "        WHEN h.pc_one IN ('Y') THEN 'Loft'\n",
    "        WHEN h.pc_one IN ('Z') THEN 'Corona'\n",
    "        END as \"location\"\n",
    "    FROM \n",
    "        custom.cth_historical_all_1718_2122 h\n",
    "    WHERE \n",
    "        h.season in ('2019-20', '2021-22', '2022-23')\n",
    "        and h.comp_seats != 0)\n",
    "\n",
    "SELECT \n",
    "    g.tier, a.location, sum(a.comp_seats)/count(distinct(a.event_date)) as paid_seats, sum(a.did_attend)/count(distinct(a.event_date)) as did_attend,\n",
    "    sum(a.did_attend)/sum(a.comp_seats)\n",
    "FROM \n",
    "    a\n",
    "LEFT JOIN \n",
    "    custom.cth_game_descriptions g on a.event_date = g.event_date\n",
    "GROUP BY \n",
    "    tier, location\n",
    "ORDER BY \n",
    "    tier, location\"\"\"\n",
    "\n",
    "comp_seats = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['predicted_attendance'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m y \u001b[39m=\u001b[39m df[[\u001b[39m'\u001b[39m\u001b[39mnum_parking_transactions\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     68\u001b[0m \u001b[39m#Test Data \u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m xdf2 \u001b[39m=\u001b[39m games[[\u001b[39m'\u001b[39;49m\u001b[39mpredicted_attendance\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mweekend\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtier_num\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[0;32m     71\u001b[0m \u001b[39m#Scaling Data\u001b[39;00m\n\u001b[0;32m     72\u001b[0m poly \u001b[39m=\u001b[39m PolynomialFeatures(degree\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, include_bias\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\riffere\\Documents\\pandas_party\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:3767\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3765\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3766\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3767\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3769\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3770\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\riffere\\Documents\\pandas_party\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5877\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5874\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   5875\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5877\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   5879\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   5880\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5881\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\riffere\\Documents\\pandas_party\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:5941\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5938\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   5940\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 5941\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['predicted_attendance'] not in index\""
     ]
    }
   ],
   "source": [
    "# 5. Predicted Parking\n",
    "\n",
    "# polynomial model (built by Pavan), multiply by avg atp to get rev\n",
    "\n",
    "q = \"\"\"\n",
    "WITH attendance as\n",
    "    (SELECT \n",
    "        season, event_date, sum(did_attend) as attendance\n",
    "    FROM \n",
    "        custom.cth_historical_all_1718_2122\n",
    "    GROUP BY \n",
    "        event_date, season\n",
    "    UNION\n",
    "    SELECT \n",
    "        '2021-22' as season, event_date, sum(num_seats) as attendance\n",
    "    FROM \n",
    "        custom.cth_playoffs_2122_expanded\n",
    "    GROUP BY \n",
    "        event_date\n",
    "    UNION\n",
    "    SELECT \n",
    "        '2022-23' as season, event_date, sum(entry) as attendance\n",
    "    FROM \n",
    "        custom.cth_attendance_scans_2223\n",
    "    GROUP BY \n",
    "        event_date\n",
    "    ORDER BY \n",
    "        event_date)\n",
    "\n",
    "SELECT \n",
    "    attendance.event_date, tier, coalesce(attendance,0) as ticket_scans, \n",
    "    coalesce(sum(paid_amount),0) as parking_paid_amount, count(*)-1 as num_parking_transactions\n",
    "FROM \n",
    "    attendance\n",
    "LEFT JOIN \n",
    "    custom.parkhub_reporting on attendance.event_date = parkhub_reporting.event_date\n",
    "LEFT JOIN \n",
    "    custom.cth_game_descriptions on attendance.event_date = cth_game_descriptions.event_date\n",
    "    --and df.time_bucket = attendance.action_time_bucket\n",
    "WHERE \n",
    "    attendance.event_date > '2022-10-01'\n",
    "    and event_time != ''\n",
    "    and tier in ('A','B','C','D','E')\n",
    "    and attendance.event_date != '2022-11-09'\n",
    "GROUP BY \n",
    "    attendance.event_date, tier, ticket_scans\n",
    "ORDER BY \n",
    "    attendance.event_date, tier\n",
    "\"\"\"\n",
    "\n",
    "df = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)\n",
    "\n",
    "#Historical Parking Data Clean Up \n",
    "df['weekend'] = df.apply(lambda row: 1 if datetime.weekday(datetime.strptime(row['event_date'], '%Y-%m-%d').date()) == 5 \n",
    "    or datetime.weekday(datetime.strptime(row['event_date'], '%Y-%m-%d').date()) == 6 else 0, axis = 1)\n",
    "df['tier_num'] = df.apply(lambda row: 5 if row['tier'] == 'A' else (4 if row['tier'] == 'B' else (3 if row['tier'] == 'C' else \n",
    "    (2 if row['tier'] == 'D' else (1 if row['tier'] == 'E' else 0)))), axis = 1)\n",
    "\n",
    "#2023-2024 Season Data Clean Up \n",
    "games['weekend'] = games.apply(lambda row: 1 if row['dow']=='Sat'  or row['dow']=='Sun'  else 0 , axis=1)\n",
    "games['tier_num'] = games.apply(lambda row: 5 if row['Tier'] == 'A' else (4 if row['Tier'] == 'B' else (3 if row['Tier'] == 'C' else \n",
    "    (2 if row['Tier'] == 'D' else (1 if row['Tier'] == 'E' else 0)))), axis = 1)\n",
    "\n",
    "#Training Data \n",
    "xdf = df[['ticket_scans', 'weekend', 'tier_num']]\n",
    "y = df[['num_parking_transactions']]\n",
    "\n",
    "#Test Data \n",
    "xdf2 = games[['predicted_attendance', 'weekend', 'tier_num']] ## GET PREDICTED ATTENDANCE FROM ABOVE CODE!\n",
    "\n",
    "#Scaling Data\n",
    "poly = PolynomialFeatures(degree=4, include_bias=False)\n",
    "poly_features2 = poly.fit_transform(xdf2)\n",
    "poly_features = poly.fit_transform(xdf)\n",
    "\n",
    "polynomial = LinearRegression().fit(poly_features, np.array(y).ravel())\n",
    "predicted = polynomial.predict(poly_features2)\n",
    "\n",
    "games['predicted_parking'] = predicted\n",
    "games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tier</th>\n",
       "      <th>fandb_percap</th>\n",
       "      <th>merch_percap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>26.172843</td>\n",
       "      <td>7.220609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>27.047258</td>\n",
       "      <td>8.899848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>25.606716</td>\n",
       "      <td>11.608013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>25.638193</td>\n",
       "      <td>9.938693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>24.078619</td>\n",
       "      <td>9.992618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P1</td>\n",
       "      <td>36.795139</td>\n",
       "      <td>7.613921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P2</td>\n",
       "      <td>39.352101</td>\n",
       "      <td>13.130252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P3</td>\n",
       "      <td>40.936511</td>\n",
       "      <td>17.943774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PS</td>\n",
       "      <td>20.883997</td>\n",
       "      <td>10.270747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SC</td>\n",
       "      <td>51.595442</td>\n",
       "      <td>34.260983</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tier  fandb_percap  merch_percap\n",
       "0    A     26.172843      7.220609\n",
       "1    B     27.047258      8.899848\n",
       "2    C     25.606716     11.608013\n",
       "3    D     25.638193      9.938693\n",
       "4    E     24.078619      9.992618\n",
       "5   P1     36.795139      7.613921\n",
       "6   P2     39.352101     13.130252\n",
       "7   P3     40.936511     17.943774\n",
       "8   PS     20.883997     10.270747\n",
       "9   SC     51.595442     34.260983"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. Predicted F&B & Merch\n",
    "\n",
    "# avg F&B & Merch per caps by tier\n",
    "\n",
    "q = \"\"\"\n",
    "with a as\n",
    "    (select \n",
    "        bypass_orderitems_2223.event_date, sum(line_item_gross_revenue) as fandb_rev\n",
    "    from \n",
    "        custom.bypass_orderitems_2223\n",
    "    where \n",
    "        event_type = 'Hockey'\n",
    "    group by \n",
    "        bypass_orderitems_2223.event_date\n",
    "    order by \n",
    "        bypass_orderitems_2223.event_date),\n",
    "b as\n",
    "    (select \n",
    "        event_date, sum(entry) as entry\n",
    "     from \n",
    "        custom.cth_attendance_scans_2223\n",
    "     group by \n",
    "        event_date\n",
    "     UNION\n",
    "     select \n",
    "        event_date, sum(entry) as entry\n",
    "     from \n",
    "        custom.cth_attendance_scans_playoffs_2223\n",
    "     group by \n",
    "        event_date),\n",
    "c as\n",
    "    (select \n",
    "        event_date, sum(line_item_gross_revenue) as merch_rev\n",
    "     from \n",
    "        custom.retailpro_invoice_items\n",
    "     group by   \n",
    "        event_date),\n",
    "\n",
    "temp as\n",
    "    (select \n",
    "        a.event_date, tier, fandb_rev, entry, merch_rev, fandb_rev/entry as fandb_percap, merch_rev/entry as merch_percap\n",
    "    from \n",
    "        a\n",
    "    left join \n",
    "        b on a.event_date = b.event_date\n",
    "    left join \n",
    "        c on a.event_date = c.event_date\n",
    "    left join \n",
    "        custom.cth_game_descriptions on a.event_date = cth_game_descriptions.event_date\n",
    "    where \n",
    "        a.event_date not like '2022-11-15')\n",
    "\n",
    "select \n",
    "    tier, avg(fandb_percap) as fandb_percap, avg(merch_percap) as merch_percap\n",
    "from \n",
    "    temp\n",
    "group by \n",
    "    tier\n",
    "order by \n",
    "    tier\n",
    "\"\"\"\n",
    "\n",
    "df = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. All-in-total\n",
    "\n",
    "# ticket+parking+f&b+merch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
