{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22518629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prefect.blocks.system import Secret\n",
    "from catnip.fla_redshift import FLA_Redshift\n",
    "from typing import Dict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import date\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import polars as pl\n",
    "import pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124f244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redshift_credentials() -> Dict:\n",
    "\n",
    "    cred_dict = {\n",
    "        \"dbname\": Secret.load(\"stellar-redshift-db-name\").get(),\n",
    "        \"host\": Secret.load(\"stellar-redshift-host\").get(),\n",
    "        \"port\": 5439,\n",
    "        \"user\": Secret.load(\"stellar-redshift-user-name\").get(),\n",
    "        \"password\": Secret.load(\"stellar-redshift-password\").get(),\n",
    "\n",
    "        \"aws_access_key_id\": Secret.load(\"fla-s3-aws-access-key-id-east-1\").get(),\n",
    "        \"aws_secret_access_key\": Secret.load(\"fla-s3-aws-secret-access-key-east-1\").get(),\n",
    "        \"bucket\": Secret.load(\"fla-s3-bucket-name-east-1\").get(),\n",
    "        \"subdirectory\": \"us-east-1\",\n",
    "\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    return cred_dict\n",
    "\n",
    "with ThreadPoolExecutor(1) as pool:\n",
    "    rs_creds = pool.submit(lambda: get_redshift_credentials()).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69e2cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tickets, Nightly Suites, Turnstile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c481ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get past singles data\n",
    "\n",
    "q = \"\"\"\n",
    "WITH playoffs_22_23 AS (\n",
    "    SELECT\n",
    "        '2022-23' AS season,\n",
    "        LEFT(RIGHT(event_name, 4), 2) AS round,\n",
    "        event_name,\n",
    "        date(event_date) as event_date,\n",
    "        CASE\n",
    "            WHEN DATEDIFF('days', DATE(add_datetime), DATE(event_date)) >= 0\n",
    "                THEN DATEDIFF('days', DATE(add_datetime), DATE(event_date))\n",
    "            ELSE 0\n",
    "        END AS days_out,\n",
    "        SUM(block_purchase_price) AS gross_revenue,\n",
    "        SUM(paid_seats) AS paid_seats\n",
    "    FROM\n",
    "        custom.cth_ticket_expanded_all_playoffs_2223\n",
    "    WHERE\n",
    "        event_name IN ('23POR1G1', '23POR1G2', '23POR1G3', '23POR2G1', '23POR2G2', '23POR3G1', '23POR3G2', '23POR4G1', '23POR4G2')\n",
    "        AND ticket_type IN ('Singles')\n",
    "    GROUP BY\n",
    "        event_name,\n",
    "        event_date,\n",
    "        days_out,\n",
    "        ticket_type\n",
    "),\n",
    "playoffs_23_24 AS (\n",
    "    SELECT\n",
    "        '2023-24' AS season,\n",
    "        RIGHT(LEFT(product_description, 6), 2) AS round,\n",
    "        LEFT(product_description, 8) AS event_name,\n",
    "        date(event_datetime) as event_date,\n",
    "        CASE\n",
    "            WHEN DATEDIFF('days', DATE(transaction_date), DATE(event_datetime)) >= 0\n",
    "                THEN DATEDIFF('days', DATE(transaction_date), DATE(event_datetime))\n",
    "            ELSE 0\n",
    "        END AS days_out,\n",
    "        SUM(gross_revenue) AS gross_revenue,\n",
    "        SUM(paid_seats) AS paid_seats\n",
    "    FROM\n",
    "        custom.cth_v_ticket_2324_playoffs\n",
    "    WHERE\n",
    "        ticket_type_playoffs IN ('Singles')\n",
    "        AND event_name != '23-24 Pl'\n",
    "    GROUP BY\n",
    "        product_description,\n",
    "        event_date,\n",
    "        days_out,\n",
    "        ticket_type_playoffs\n",
    "),\n",
    "playoffs_24_25 AS (\n",
    "    SELECT\n",
    "        '2024-25' AS season,\n",
    "        RIGHT(LEFT(product_description, 6), 2) AS round,\n",
    "        LEFT(product_description, 8) AS event_name,\n",
    "        date(event_datetime) as event_date,\n",
    "        CASE\n",
    "            WHEN DATEDIFF('days', DATE(transaction_date), DATE(event_datetime)) >= 0\n",
    "                THEN DATEDIFF('days', DATE(transaction_date), DATE(event_datetime))\n",
    "            ELSE 0\n",
    "        END AS days_out,\n",
    "        SUM(gross_revenue) AS gross_revenue,\n",
    "        SUM(paid_seats) AS paid_seats\n",
    "    FROM\n",
    "        custom.cth_v_ticket_2425_playoffs\n",
    "    WHERE\n",
    "        ticket_type_playoffs IN ('Singles')\n",
    "        AND event_name != '24-25 Pl'\n",
    "    GROUP BY\n",
    "        product_description,\n",
    "        event_date,\n",
    "        days_out,\n",
    "        ticket_type_playoffs\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    playoffs_22_23\n",
    "UNION ALL\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    playoffs_23_24\n",
    "UNION ALL\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    playoffs_24_25\n",
    "ORDER BY\n",
    "    season,\n",
    "    round,\n",
    "    event_name,\n",
    "    days_out DESC\n",
    "\"\"\"\n",
    "\n",
    "ticket_df = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "511c32c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\2629162886.py:5: FutureWarning: The 'axis' keyword in DataFrame.groupby is deprecated and will be removed in a future version.\n",
      "  cumdf = ticket_df.groupby(by = ['round','event_name','event_date'], axis = 0)[['gross_revenue','paid_seats']].cumsum().rename(columns = {'gross_revenue':'cum_gross_rev', 'paid_seats':'cum_num_seats'})\n"
     ]
    }
   ],
   "source": [
    "# get average tickets sold by days out from previous seasons\n",
    "\n",
    "ticket_df['min_days_out'] = ticket_df.apply(lambda row: ticket_df[(ticket_df['event_name'] == row['event_name'])]['days_out'].min(), axis = 1)\n",
    "\n",
    "cumdf = ticket_df.groupby(by = ['round','event_name','event_date'], axis = 0)[['gross_revenue','paid_seats']].cumsum().rename(columns = {'gross_revenue':'cum_gross_rev', 'paid_seats':'cum_num_seats'})\n",
    "\n",
    "ticket_df = pd.concat([ticket_df,cumdf], axis = 1)\n",
    "\n",
    "ticket_df['final_seats'] = ticket_df.apply(lambda row: ticket_df[(ticket_df['event_name'] == row['event_name'])&\n",
    "                                                  (ticket_df['days_out'] == row['min_days_out'])]['cum_num_seats'].item(), axis = 1)\n",
    "\n",
    "ticket_df['per_seats_in'] = [x/y for x,y in zip(ticket_df['cum_num_seats'],ticket_df['final_seats'])]\n",
    "\n",
    "ticket_df['final_rev'] = ticket_df.apply(lambda row: ticket_df[(ticket_df['event_name'] == row['event_name'])&\n",
    "                                                  (ticket_df['days_out'] == row['min_days_out'])]['cum_gross_rev'].item(), axis = 1)\n",
    "\n",
    "ticket_df['per_rev_in'] = [x/y for x,y in zip(ticket_df['cum_gross_rev'],ticket_df['final_rev'])]\n",
    "\n",
    "ticket_df = ticket_df[['season','round', 'event_name', 'event_date','days_out','gross_revenue','paid_seats', 'cum_gross_rev','cum_num_seats','per_seats_in','per_rev_in']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2273bea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge averages back to 24/25 season\n",
    "\n",
    "df_train = ticket_df[ticket_df['season'] != '2024-25']\n",
    "\n",
    "df_2425 = ticket_df[ticket_df['season'] == '2024-25']\n",
    "\n",
    "df_avgs = df_train.groupby(by = ['round','days_out'])[['per_seats_in','per_rev_in']].mean().rename(columns = {'per_seats_in':'avg_per_seats_in','per_rev_in':'avg_per_rev_in'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93cd5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict final singles totals for 24/25\n",
    "\n",
    "df_merged = df_2425.merge(right = df_avgs, how = 'left', on = ['round','days_out'])\n",
    "\n",
    "df_merged['paid_seats'] = df_merged['cum_num_seats']/df_merged['avg_per_seats_in']\n",
    "\n",
    "df_merged['gross_revenue'] = df_merged['cum_gross_rev']/df_merged['avg_per_rev_in']\n",
    "\n",
    "min_indices = df_merged.groupby('event_name')['days_out'].idxmin()\n",
    "\n",
    "result = df_merged.loc[min_indices]\n",
    "\n",
    "result['ticket_type_playoffs'] = 'Singles'\n",
    "\n",
    "result['tier'] = result['event_name'].str[-4:].str[:2]\n",
    "\n",
    "result = result[['event_name','event_date','tier','ticket_type_playoffs','paid_seats','gross_revenue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d87583ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current in from other ticket types\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "    LEFT(product_description, 8) AS event_name,\n",
    "    RIGHT(LEFT(product_description,6),2) AS tier,\n",
    "    date(event_datetime) as event_date,\n",
    "    ticket_type_playoffs,\n",
    "    sum(gross_revenue) as gross_revenue,\n",
    "    sum(paid_seats) as paid_seats\n",
    "FROM\n",
    "    custom.cth_v_ticket_2425_playoffs\n",
    "WHERE\n",
    "    date(event_datetime) <= '2025-05-02'\n",
    "    and ticket_type_playoffs != 'Singles'\n",
    "GROUP BY\n",
    "    product_description,\n",
    "    event_date,\n",
    "    ticket_type_playoffs\n",
    "\"\"\"\n",
    "\n",
    "current_in = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8826198e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\3898573725.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  paid_tiers = show_rate.groupby(by = ['tier']).apply(weighted_paid_average).reset_index()\n",
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\3898573725.py:85: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  comp_tiers = show_rate.groupby(by = ['tier']).apply(weighted_comp_average).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# get weighted show_rate avergaes over last 4 seasons\n",
    "\n",
    "# get historical show rate data\n",
    "\n",
    "q = \"\"\"\n",
    "WITH historical AS (\n",
    "    SELECT\n",
    "        game_desc.season,\n",
    "        game_desc.tier,\n",
    "        ticket.event_date::date,\n",
    "        ticket.comp_seats::float,\n",
    "        ticket.paid_seats::float,\n",
    "        CASE\n",
    "            WHEN ticket.is_comp = TRUE AND ticket.did_attended = TRUE THEN 1\n",
    "            ELSE 0\n",
    "        END AS \"comp_seats_attended\",\n",
    "        CASE\n",
    "            WHEN is_comp = FALSE AND did_attended = TRUE THEN 1\n",
    "            ELSE 0\n",
    "        END AS \"paid_seats_attended\"\n",
    "    FROM\n",
    "        custom.cth_v_historical_ticket ticket\n",
    "    INNER JOIN\n",
    "        custom.cth_game_descriptions game_desc\n",
    "            ON ticket.event_datetime::date = game_desc.event_datetime::date\n",
    "            AND game_desc.season IN ('2021-22', '2022-23', '2023-24', '2024-25')\n",
    "            AND game_desc.event_datetime < current_date\n",
    "),\n",
    "tier_show_rate AS (\n",
    "    SELECT\n",
    "        season,\n",
    "        tier,\n",
    "        sum(historical.comp_seats_attended)::float / nullif(sum(historical.comp_seats),0) AS \"comp_show_rate\",\n",
    "        sum(historical.paid_seats_attended)::float / nullif(sum(historical.paid_seats),0) AS \"paid_show_rate\"\n",
    "    FROM\n",
    "        historical\n",
    "    GROUP BY\n",
    "        season,\n",
    "        tier\n",
    ")\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    tier_show_rate\n",
    "where\n",
    "    tier in ('R1','R2','R3','SC')\n",
    "\"\"\"\n",
    "\n",
    "show_rate = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)\n",
    "\n",
    "weights = {'2022-23': .5, '2023-24':1,'2024-25':1.5}\n",
    "\n",
    "show_rate['weights'] = show_rate['season'].map(weights)\n",
    "\n",
    "def weighted_paid_average(group):\n",
    "    # Calculate the weighted sum\n",
    "    weighted_sum = (group['paid_show_rate'] * group['weights']).sum()\n",
    "    \n",
    "    # Calculate the weight sum\n",
    "    weight_sum = group['weights'].sum()\n",
    "    \n",
    "    # Calculate the weighted average\n",
    "    wavg = weighted_sum / weight_sum\n",
    "    \n",
    "    return pd.Series({\n",
    "        'weighted_paid_average': wavg\n",
    "    })\n",
    "\n",
    "paid_tiers = show_rate.groupby(by = ['tier']).apply(weighted_paid_average).reset_index()\n",
    "\n",
    "def weighted_comp_average(group):\n",
    "    # Calculate the weighted sum\n",
    "    weighted_sum = (group['comp_show_rate'] * group['weights']).sum()\n",
    "    \n",
    "    # Calculate the weight sum\n",
    "    weight_sum = group['weights'].sum()\n",
    "    \n",
    "    # Calculate the weighted average\n",
    "    wavg = weighted_sum / weight_sum\n",
    "    \n",
    "    return pd.Series({\n",
    "        'weighted_comp_average': wavg\n",
    "    })\n",
    "\n",
    "comp_tiers = show_rate.groupby(by = ['tier']).apply(weighted_comp_average).reset_index()\n",
    "\n",
    "tiers = pd.merge(paid_tiers, comp_tiers, on = ['tier'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f46b2181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge onto 24/25 data and predict attendance\n",
    "\n",
    "df_final = pd.concat([result,current_in])\n",
    "\n",
    "final_tickets_and_attendance = df_final.groupby(by = ['event_name','event_date', 'tier'])[['paid_seats','gross_revenue']].sum().reset_index()\n",
    "\n",
    "final_tickets_and_attendance = final_tickets_and_attendance.merge(tiers, how = 'left', on = 'tier')\n",
    "\n",
    "final_tickets_and_attendance['total_attendance'] = final_tickets_and_attendance['paid_seats'] * final_tickets_and_attendance['weighted_paid_average']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6822c33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tickets_and_attendance = final_tickets_and_attendance[['event_name','event_date','tier','paid_seats','gross_revenue','total_attendance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e2b08b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nightly suite tickets\n",
    "\n",
    "q = \"\"\"\n",
    "WITH sold_suites AS (\n",
    "\n",
    "    WITH comp_temp AS (\n",
    "        SELECT\n",
    "            product_id,\n",
    "            section,\n",
    "            product_id || '-' || section AS \"id\",\n",
    "            'COMP'::varchar AS \"status\",\n",
    "            NULL::varchar AS \"locks\",\n",
    "            'Comp'::varchar AS \"allocations\",\n",
    "            sum(gross_revenue) AS \"gross_revenue\"\n",
    "        FROM\n",
    "            custom.cth_v_ticket_status_2425_playoffs\n",
    "        WHERE\n",
    "            (pc_one IN ('U', 'V', 'W') OR section = 'House')\n",
    "            AND status = 'SOLD'\n",
    "        GROUP BY\n",
    "            product_id,\n",
    "            section\n",
    "        HAVING\n",
    "            sum(gross_revenue) = 0\n",
    "    )\n",
    "\n",
    "    -- sold suites\n",
    "    SELECT\n",
    "        product_id,\n",
    "        section,\n",
    "        product_id || '-' || section AS \"id\",\n",
    "        'SOLD'::varchar AS \"status\",\n",
    "        NULL::varchar AS \"locks\",\n",
    "        'Sold'::varchar AS \"allocations\",\n",
    "        sum(gross_revenue) AS \"gross_revenue\"\n",
    "    FROM\n",
    "        custom.cth_v_ticket_status_2425_playoffs\n",
    "    WHERE\n",
    "        (pc_one IN ('U', 'V', 'W') OR section = 'House')\n",
    "        AND status = 'SOLD'\n",
    "    GROUP BY\n",
    "        product_id,\n",
    "        section\n",
    "    HAVING\n",
    "        sum(gross_revenue) > 0\n",
    "    UNION ALL\n",
    "\n",
    "    -- killed suites\n",
    "    SELECT\n",
    "        product_id,\n",
    "        section,\n",
    "        product_id || '-' || section AS \"id\",\n",
    "        'SOLD'::varchar AS \"status\",\n",
    "        NULL::varchar AS \"locks\",\n",
    "        'Sold'::varchar AS \"allocations\",\n",
    "        sum(gross_revenue) AS \"gross_revenue\"\n",
    "    FROM\n",
    "        custom.cth_v_ticket_status_2425_playoffs\n",
    "    WHERE\n",
    "        (\n",
    "            pc_one IN ('U', 'V', 'W')\n",
    "            OR section = 'House'\n",
    "        )\n",
    "        AND (\n",
    "            allocations ilike '%kill%'\n",
    "            OR locks ilike '%kill%'\n",
    "            OR allocations ilike '%panthers players%'\n",
    "            OR allocations ilike '%owner%'\n",
    "            OR allocations ilike '%hockey operations%'\n",
    "            OR allocations ilike '%visiting team%'\n",
    "        )\n",
    "        AND \"id\" NOT IN (SELECT ct.id FROM comp_temp ct)\n",
    "    GROUP BY\n",
    "        product_id,\n",
    "        section\n",
    "    UNION ALL\n",
    "\n",
    "    -- comp suites\n",
    "    SELECT * FROM comp_temp\n",
    "),\n",
    "held_suites AS (\n",
    "    SELECT\n",
    "        product_id,\n",
    "        section,\n",
    "        product_id || '-' || section AS \"id\",\n",
    "        'HELD'::varchar AS \"status\",\n",
    "        NULL::varchar AS \"locks\",\n",
    "        'Held'::varchar AS \"allocations\",\n",
    "        sum(gross_revenue) AS \"gross_revenue\"\n",
    "    FROM\n",
    "        custom.cth_v_ticket_status_2425_playoffs\n",
    "    WHERE\n",
    "        (pc_one IN ('U', 'V', 'W') OR section = 'House')\n",
    "        AND status = 'HELD'\n",
    "        AND \"id\" NOT IN (SELECT s.id FROM sold_suites s)\n",
    "    GROUP BY\n",
    "        product_id,\n",
    "        section\n",
    "),\n",
    "-- SELECT * FROM held_suites;\n",
    "available_suites AS (\n",
    "    SELECT\n",
    "        product_id,\n",
    "        section,\n",
    "        product_id || '-' || section AS \"id\",\n",
    "        'AVAIL'::varchar AS \"status\",\n",
    "        LISTAGG(DISTINCT locks, ', ') AS \"locks\",\n",
    "        LISTAGG(DISTINCT allocations, ', ') WITHIN GROUP (ORDER BY allocations) AS \"allocations\",\n",
    "        sum(gross_revenue) AS \"gross_revenue\"\n",
    "    FROM\n",
    "        custom.cth_v_ticket_status_2425_playoffs\n",
    "    WHERE\n",
    "        (pc_one IN ('U', 'V', 'W') OR section = 'House')\n",
    "        AND status = 'AVAIL'\n",
    "        AND \"id\" NOT IN (SELECT s.id FROM sold_suites s)\n",
    "        AND \"id\" NOT IN (SELECT h.id FROM held_suites h)\n",
    "        AND (allocations <> '[\"Standing Room Only\"]' OR allocations IS NULL)\n",
    "    GROUP BY\n",
    "        product_id,\n",
    "        section\n",
    "),\n",
    "temp AS (\n",
    "    SELECT * FROM sold_suites\n",
    "    UNION ALL\n",
    "    SELECT * FROM held_suites\n",
    "    UNION ALL\n",
    "    SELECT * FROM available_suites\n",
    ")\n",
    "SELECT\n",
    "    split_part(products.product_description, ' - ', 1) AS \"event_name\",\n",
    "    CASE\n",
    "        WHEN RIGHT(LEFT(product_description,6),2) = 'R4' THEN 'SC'\n",
    "        ELSE RIGHT(LEFT(product_description,6),2)\n",
    "    END AS tier,\n",
    "    event_date,\n",
    "    temp.*\n",
    "FROM\n",
    "    temp\n",
    "LEFT JOIN\n",
    "    custom.seatgeek_v_products products ON temp.product_id = products.product_id\n",
    "ORDER BY\n",
    "    \"event_name\",\n",
    "    event_date,\n",
    "    section\n",
    "\"\"\"\n",
    "\n",
    "current_nightly_suites = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49c4db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_nightly_suites.groupby(by = ['event_name','event_date','status'])[['section']].count().reset_index()\n",
    "\n",
    "current_nightly_suites = current_nightly_suites.pivot_table(index=['event_name','event_date', 'tier'], columns='status', \n",
    "             values='section', aggfunc='count').reset_index()\n",
    "\n",
    "current_nightly_suites['event_date'] = pd.to_datetime(current_nightly_suites['event_date'])\n",
    "\n",
    "current_nightly_suites['days_out_from_event'] = (current_nightly_suites['event_date'] - datetime.now()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "676d6ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    custom.forecasting_hockey_nightly_suites_playoffs\n",
    "\"\"\"\n",
    "\n",
    "forecasting_nightly_suites = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f88575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_nightly_suites = current_nightly_suites.merge(forecasting_nightly_suites, how = 'left', on = ['tier', 'days_out_from_event'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e64ecfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_nightly_suites['is_going_to_sellout'] = current_nightly_suites['cumulative_avg_sold'] > current_nightly_suites['AVAIL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e10d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_nightly_suites = current_nightly_suites[['event_name','event_date','days_out_from_event','tier','AVAIL','COMP','HELD','SOLD','cumulative_avg_sold','is_going_to_sellout']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7cb8bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merch, F&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2c629",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "WITH attendance AS (\n",
    "    SELECT\n",
    "        event_datetime,\n",
    "        COUNT(*) AS attendance\n",
    "    FROM\n",
    "        custom.cth_v_attendance_2324_playoffs\n",
    "    GROUP BY\n",
    "        event_datetime\n",
    ")\n",
    "SELECT\n",
    "    cth_game_descriptions.season,\n",
    "    cth_game_descriptions.event_date,\n",
    "    tier,\n",
    "    day_of_week,\n",
    "    start_time,\n",
    "    attendance,\n",
    "    gross_revenue,\n",
    "    num_orders,\n",
    "    quantity_sold\n",
    "FROM\n",
    "    custom.cheq_v_hockey_summary\n",
    "LEFT JOIN\n",
    "    custom.cth_game_descriptions \n",
    "    ON DATE(cheq_v_hockey_summary.event_date) = DATE(cth_game_descriptions.event_date)\n",
    "LEFT JOIN\n",
    "    attendance \n",
    "    ON DATE(attendance.event_datetime) = DATE(cheq_v_hockey_summary.event_date)\n",
    "WHERE\n",
    "    tier IN ('R1', 'R2', 'R3', 'SC')\n",
    "\"\"\"\n",
    "\n",
    "historical_f_and_b = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c7e25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_columns(df):\n",
    "\n",
    "    day_map = {\n",
    "    'Fri': 1,\n",
    "    'Sat': 1,\n",
    "    'Sun': 1,\n",
    "    'Mon': 0,\n",
    "    'Tue': 0,\n",
    "    'Wed': 0,\n",
    "    'Thu': 0\n",
    "}\n",
    "\n",
    "    df['weekend'] = df.apply(\n",
    "        lambda row: day_map.get(row['day_of_week'], 0), axis=1)\n",
    "\n",
    "    start_time_map = {\n",
    "        '12:30 PM': 1,\n",
    "        '12:45 PM': 1,\n",
    "        '1:00 PM': 1,\n",
    "        '3:00 PM': 1,\n",
    "        '3:30 PM': 1,\n",
    "        '4:00 PM': 2,\n",
    "        '5:00 PM': 2,\n",
    "        '6:00 PM': 2\n",
    "    }\n",
    "\n",
    "    df['start_time_num'] = df.apply(\n",
    "        lambda row: start_time_map.get(row['start_time'], 0),\n",
    "        axis=1)\n",
    "\n",
    "    tier_mapping = {\n",
    "        'A': 5,\n",
    "        'B': 4,\n",
    "        'C': 3,\n",
    "        'D': 2,\n",
    "        'E': 1\n",
    "    }\n",
    "\n",
    "    df['tier_num'] = df.apply(\n",
    "        lambda row: tier_mapping.get(row['tier'], 0),\n",
    "        axis=1) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c22eb31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_f_and_b = get_model_columns(historical_f_and_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ea073",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "WITH attendance AS\n",
    "    (SELECT\n",
    "        event_datetime,\n",
    "        COUNT(*) AS attendance\n",
    "    FROM\n",
    "        custom.cth_v_attendance_2324_playoffs\n",
    "    GROUP BY\n",
    "        event_datetime)\n",
    "SELECT\n",
    "    cth_game_descriptions.season,\n",
    "    cth_game_descriptions.event_date,\n",
    "    tier,\n",
    "    day_of_week,\n",
    "    start_time,\n",
    "    attendance,\n",
    "    SUM(gross_revenue) AS gross_revenue,\n",
    "    SUM(qty) AS quantity,\n",
    "    COUNT(distinct invoice_id) AS num_orders\n",
    "FROM\n",
    "    custom.retailpro_v_invoice_items\n",
    "LEFT JOIN\n",
    "    custom.cth_game_descriptions ON retailpro_v_invoice_items.event_date = cth_game_descriptions.event_date\n",
    "LEFT JOIN\n",
    "    attendance ON retailpro_v_invoice_items.event_date = date(attendance.event_datetime)\n",
    "WHERE\n",
    "    season IN ('2023-24','2024-25')\n",
    "    AND tier IN ('R1','R2','R3','SC')\n",
    "GROUP BY\n",
    "    cth_game_descriptions.season,\n",
    "    cth_game_descriptions.event_date,\n",
    "    tier,\n",
    "    is_premier,\n",
    "    original_six_plus_extra,\n",
    "    day_of_week,\n",
    "    start_time,\n",
    "    attendance\n",
    "\"\"\"\n",
    "\n",
    "historical_merch = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "39eb8486",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_merch = get_model_columns(historical_merch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "SELECT\n",
    "    season,\n",
    "    date(event_date) AS event_date,\n",
    "    day_of_week,\n",
    "    tier,\n",
    "    start_time\n",
    "FROM\n",
    "    custom.cth_game_descriptions\n",
    "WHERE\n",
    "    event_date >= current_date\n",
    "\"\"\"\n",
    "\n",
    "future_game_info = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)\n",
    "\n",
    "future_game_info = future_game_info.merge(final_tickets_and_attendance, how = 'left', on = ['event_date', 'tier'])\n",
    "\n",
    "future_game_info = get_model_columns(future_game_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3df84fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_f_and_b_model(df, df_future):\n",
    "\n",
    "    x_train = df[['attendance','weekend','start_time_num','tier_num']]\n",
    "    y_train = df[['gross_revenue']]\n",
    "\n",
    "    x_test = df_future[['total_attendance','weekend','start_time_num','tier_num']]\n",
    "\n",
    "    #scalar = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    #poly_features = scalar.fit_transform(x_train)\n",
    "    scalar = StandardScaler()\n",
    "    poly_features = scalar.fit_transform(x_train)\n",
    "\n",
    "    polynomial = LinearRegression().fit(poly_features, np.array(y_train).ravel())\n",
    "    #predicted_train = polynomial.predict(poly_features)\n",
    "\n",
    "    poly_features2 = scalar.fit_transform(x_test)\n",
    "\n",
    "    return polynomial.predict(poly_features2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fcbadb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_game_info['predicted_f_and_b_gross_rev'] = run_f_and_b_model(historical_f_and_b, future_game_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "721fe792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_merch_model(df, df_future):\n",
    "\n",
    "    x_train = df[['attendance','weekend','start_time_num','tier_num']]\n",
    "    y_train = df[['gross_revenue']]\n",
    "\n",
    "    x_test = df_future[['total_attendance','weekend','start_time_num','tier_num']]\n",
    "\n",
    "    #scalar = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    #poly_features = scalar.fit_transform(x_train)\n",
    "    scalar = StandardScaler()\n",
    "    poly_features = scalar.fit_transform(x_train)\n",
    "\n",
    "    polynomial = LinearRegression().fit(poly_features, np.array(y_train).ravel())\n",
    "    #predicted_train = polynomial.predict(poly_features)\n",
    "\n",
    "    poly_features2 = scalar.fit_transform(x_test)\n",
    "\n",
    "    return polynomial.predict(poly_features2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "631c7a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_game_info['predicted_merch_gross_rev'] = run_merch_model(historical_merch, future_game_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7dd67d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "merch_and_f_and_b_rev = future_game_info[['event_date','event_name','total_attendance','predicted_f_and_b_gross_rev', 'predicted_merch_gross_rev']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2b3f667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get historical show rates by tier and lot\n",
    "\n",
    "q = \"\"\"\n",
    "WITH prepaid AS (\n",
    "    SELECT\n",
    "        event_datetime,\n",
    "        location_group\n",
    "    FROM\n",
    "        custom.ctp_v_ticket_2324\n",
    "    WHERE\n",
    "        event_type ILIKE '%panthers%'\n",
    "        AND event_datetime < CURRENT_DATE\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        event_datetime,\n",
    "        location_group\n",
    "    FROM\n",
    "        custom.ctp_v_ticket_2425\n",
    "    WHERE\n",
    "        event_type ILIKE '%panthers%'\n",
    "        AND event_datetime < CURRENT_DATE\n",
    "),\n",
    "prepaid_agg AS (\n",
    "    SELECT\n",
    "        event_datetime,\n",
    "        location_group,\n",
    "        COUNT(*) AS prepaid_passes\n",
    "    FROM\n",
    "        prepaid\n",
    "    GROUP BY\n",
    "        event_datetime, \n",
    "        location_group\n",
    "),\n",
    "scans AS (\n",
    "    SELECT\n",
    "        season,\n",
    "        cth_game_descriptions.event_datetime,\n",
    "        tier,\n",
    "        location_group,\n",
    "        CASE\n",
    "            WHEN paid_amount = 0 THEN 1\n",
    "            ELSE 0 \n",
    "        END AS num_scans\n",
    "    FROM\n",
    "        custom.parkhub_v_transactions\n",
    "    LEFT JOIN\n",
    "        custom.cth_game_descriptions \n",
    "        ON parkhub_v_transactions.event_datetime = cth_game_descriptions.event_datetime\n",
    "    WHERE\n",
    "        cth_game_descriptions.event_datetime IS NOT NULL\n",
    "        AND season IN ('2023-24', '2024-25')\n",
    "),\n",
    "scans_agg AS (\n",
    "    SELECT\n",
    "        season,\n",
    "        event_datetime,\n",
    "        tier,\n",
    "        location_group,\n",
    "        SUM(num_scans) AS num_scans\n",
    "    FROM\n",
    "        scans\n",
    "    GROUP BY\n",
    "        season,\n",
    "        event_datetime,\n",
    "        tier,\n",
    "        location_group\n",
    ")\n",
    "SELECT\n",
    "    season,\n",
    "    prepaid_agg.event_datetime,\n",
    "    tier,\n",
    "    prepaid_agg.location_group,\n",
    "    prepaid_passes,\n",
    "    num_scans,\n",
    "    num_scans * 1.0 / prepaid_passes::FLOAT AS show_rate\n",
    "FROM\n",
    "    scans_agg\n",
    "LEFT JOIN\n",
    "    prepaid_agg \n",
    "    ON scans_agg.event_datetime = prepaid_agg.event_datetime\n",
    "    AND scans_agg.location_group = prepaid_agg.location_group\n",
    "WHERE\n",
    "    prepaid_agg.event_datetime IS NOT NULL\n",
    "    AND tier IN ('R1','R2','R3','SC')\n",
    "\"\"\"\n",
    "\n",
    "show_rate_df = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "56aa7e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\3821290929.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  paid_tiers = show_rate_df.groupby(by = ['tier','location_group']).apply(weighted_paid_average).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# weigh this past season more heavily\n",
    "\n",
    "weights = {'2023-24':1.25,'2024-25':2.75}\n",
    "\n",
    "show_rate_df['weights'] = show_rate_df['season'].map(weights)\n",
    "\n",
    "def weighted_paid_average(group):\n",
    "    # Calculate the weighted sum\n",
    "    weighted_sum = (group['show_rate'] * group['weights']).sum()\n",
    "    \n",
    "    # Calculate the weight sum\n",
    "    weight_sum = group['weights'].sum()\n",
    "    \n",
    "    # Calculate the weighted average\n",
    "    wavg = weighted_sum / weight_sum\n",
    "    \n",
    "    return pd.Series({\n",
    "        'weighted_average': wavg\n",
    "    })\n",
    "\n",
    "paid_tiers = show_rate_df.groupby(by = ['tier','location_group']).apply(weighted_paid_average).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3e77dc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather historical prepaid parking data\n",
    "\n",
    "q = \"\"\"\n",
    "WITH prepaid AS (\n",
    "    SELECT\n",
    "        event_datetime,\n",
    "        location_group,\n",
    "        DATE(transaction_date) AS transaction_date\n",
    "    FROM\n",
    "        custom.ctp_v_ticket_2324\n",
    "    WHERE\n",
    "        event_type ILIKE '%panthers%'\n",
    "        AND event_datetime < CURRENT_DATE\n",
    "    UNION ALL\n",
    "    SELECT\n",
    "        event_datetime,\n",
    "        location_group,\n",
    "        DATE(transaction_date) AS transaction_date\n",
    "    FROM\n",
    "        custom.ctp_v_ticket_2425\n",
    "    WHERE\n",
    "        event_type ILIKE '%panthers%'\n",
    "        AND event_datetime < CURRENT_DATE\n",
    ")\n",
    "SELECT\n",
    "    DATE(prepaid.event_datetime) AS event_date,\n",
    "    location_group,\n",
    "    'prepaid' AS parking_type,\n",
    "    DATEDIFF('days', transaction_date, prepaid.event_datetime) AS days_out,\n",
    "    CASE\n",
    "        WHEN DATEDIFF('days', transaction_date, prepaid.event_datetime) >= 150 THEN 0\n",
    "        ELSE COUNT(*)\n",
    "    END AS num_passes\n",
    "FROM\n",
    "    prepaid\n",
    "LEFT JOIN\n",
    "    custom.cth_game_descriptions \n",
    "    ON prepaid.event_datetime = cth_game_descriptions.event_datetime\n",
    "WHERE\n",
    "    DATEDIFF('days', transaction_date, prepaid.event_datetime) >= 0\n",
    "    AND tier IN ('R1', 'R2', 'R3', 'SC')\n",
    "GROUP BY\n",
    "    prepaid.event_datetime,\n",
    "    location_group,\n",
    "    transaction_date\n",
    "ORDER BY\n",
    "    prepaid.event_datetime,\n",
    "    location_group,\n",
    "    transaction_date DESC\n",
    "\"\"\"\n",
    "\n",
    "historical_prepaid_parking_info = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)\n",
    "\n",
    "location_map = {\n",
    "    'Club': 4,\n",
    "    'Garage': 3,\n",
    "    'General': 2,\n",
    "    'Valet': 1\n",
    "}\n",
    "\n",
    "historical_prepaid_parking_info['location_num'] = historical_prepaid_parking_info.apply(\n",
    "    lambda row: location_map.get(row['location_group'], 0), axis=1)\n",
    "\n",
    "historical_prepaid_parking_info['cumulative_num_passes']  = historical_prepaid_parking_info.groupby(['event_date', 'location_group'])['num_passes'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "528d7500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather historical game data (ie tier, dow, and start time)\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "    date(cth_game_descriptions.event_date) AS event_date,\n",
    "    tier,\n",
    "    day_of_week,\n",
    "    start_time,\n",
    "    total_tickets\n",
    "FROM\n",
    "    custom.cth_v_historical_attendance_summary\n",
    "LEFT JOIN\n",
    "    custom.cth_game_descriptions ON cth_v_historical_attendance_summary.event_date = cth_game_descriptions.event_date\n",
    "WHERE\n",
    "    tier IN ('R1','R2','R3','SC')\n",
    "\"\"\"\n",
    "\n",
    "all_game_info = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)\n",
    "\n",
    "day_map = {\n",
    "    'Fri': 1,\n",
    "    'Sat': 1,\n",
    "    'Sun': 1,\n",
    "    'Mon': 0,\n",
    "    'Tue': 0,\n",
    "    'Wed': 0,\n",
    "    'Thu': 0\n",
    "}\n",
    "\n",
    "all_game_info['weekend'] = all_game_info.apply(\n",
    "    lambda row: day_map.get(row['day_of_week'], 0), axis=1)\n",
    "\n",
    "start_time_map = {\n",
    "    '12:30 PM': 1,\n",
    "    '12:45 PM': 1,\n",
    "    '1:00 PM': 1,\n",
    "    '3:00 PM': 1,\n",
    "    '3:30 PM': 1,\n",
    "    '4:00 PM': 2,\n",
    "    '5:00 PM': 2,\n",
    "    '6:00 PM': 2\n",
    "}\n",
    "\n",
    "all_game_info['start_time_num'] = all_game_info.apply(\n",
    "    lambda row: start_time_map.get(row['start_time'], 0),\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "046cb2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather upcoming game data including current prepaid totals by game and lot\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "    DATE(cth_game_descriptions.event_datetime) AS event_date,\n",
    "    DATEDIFF('day', CURRENT_DATE, cth_game_descriptions.event_datetime) AS days_out,\n",
    "    ctp_v_ticket_2425.location_group,\n",
    "    capacity::INT,\n",
    "    SUM(paid_seats) + SUM(comp_seats) AS prepaid_cars,\n",
    "    SUM(gross_revenue) AS current_gross_revenue,\n",
    "    capacity::INT - (SUM(paid_seats) + SUM(comp_seats)) AS cap_remaining\n",
    "FROM\n",
    "    custom.ctp_v_ticket_2425\n",
    "LEFT JOIN\n",
    "    custom.ctp_parking_capacities \n",
    "    ON ctp_v_ticket_2425.location_group = ctp_parking_capacities.location_group\n",
    "LEFT JOIN\n",
    "    custom.cth_game_descriptions \n",
    "    ON ctp_v_ticket_2425.event_datetime = cth_game_descriptions.event_datetime\n",
    "WHERE\n",
    "    cth_game_descriptions.event_datetime IS NOT NULL \n",
    "    AND cth_game_descriptions.event_datetime >= CURRENT_DATE\n",
    "GROUP BY\n",
    "    cth_game_descriptions.event_datetime,\n",
    "    ctp_v_ticket_2425.location_group,\n",
    "    capacity\n",
    "ORDER BY\n",
    "    cth_game_descriptions.event_datetime,\n",
    "    ctp_v_ticket_2425.location_group\n",
    "\"\"\"\n",
    "\n",
    "upcoming_game_info = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24488c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather upcoming pricing data by game and lot\n",
    "\n",
    "q = \"\"\"\n",
    "SELECT\n",
    "    tier,\n",
    "    location_group,\n",
    "    max(transaction_date) AS \"transaction_date\",\n",
    "    max(adjusted_price) AS \"highest_price\"\n",
    "FROM\n",
    "    custom.ctp_v_ticket_2425\n",
    "LEFT JOIN             \n",
    "    custom.cth_game_descriptions on ctp_v_ticket_2425.event_datetime = cth_game_descriptions.event_datetime\n",
    "WHERE\n",
    "    is_comp = FALSE\n",
    "    AND price_type ILIKE 'IA%'\n",
    "GROUP BY\n",
    "    tier, \n",
    "    location_group\n",
    "\"\"\"\n",
    "\n",
    "pricing_info = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2729752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model to predict remaining prepaid cars\n",
    "\n",
    "def run_prepaid_model(df, df_future, lot):\n",
    "\n",
    "    total_table = df[df['location_group'] == lot]\n",
    "\n",
    "    x_train = total_table[['days_out','weekend','start_time_num']]\n",
    "    y_train = total_table[['cumulative_num_passes']]\n",
    "\n",
    "    total_future_table = df_future[df_future['location_group'] == lot]\n",
    "\n",
    "    x_test = total_future_table[['days_out','weekend','start_time_num']]\n",
    "\n",
    "    #scalar = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    #poly_features = scalar.fit_transform(x_train)\n",
    "    scalar = StandardScaler()\n",
    "    poly_features = scalar.fit_transform(x_train)\n",
    "\n",
    "    polynomial = LinearRegression().fit(poly_features, np.array(y_train).ravel())\n",
    "    #predicted_train = polynomial.predict(poly_features)\n",
    "\n",
    "    poly_features2 = scalar.fit_transform(x_test)\n",
    "\n",
    "    return polynomial.predict(poly_features2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "daa7cdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge hisotrical prepaid parking data with hisorical game data for training model\n",
    "\n",
    "total_table = historical_prepaid_parking_info.merge(all_game_info, how = 'left', on = 'event_date')\n",
    "total_table = total_table[total_table['tier'].isin(['R1','R2','R3','SC'])]\n",
    "\n",
    "# merge upcoming parking data with hisorical game data for testing model\n",
    "\n",
    "total_future_table = upcoming_game_info.merge(future_game_info, how = 'left', on = 'event_date')\n",
    "total_future_table = total_future_table[total_future_table['tier'].isin(['R1','R2','R3','SC'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e7351843",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\484755223.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp['predicted_parking'] = run_prepaid_model(total_table, total_future_table, lot)\n",
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\484755223.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp['predicted_parking'] = run_prepaid_model(total_table, total_future_table, lot)\n"
     ]
    }
   ],
   "source": [
    "# only predicting for general and garage not  club, valet or executive\n",
    "\n",
    "lots = ['General','Garage']\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for lot in lots:\n",
    "\n",
    "    temp = total_future_table[total_future_table['location_group'] == lot]\n",
    "\n",
    "    temp['predicted_parking'] = run_prepaid_model(total_table, total_future_table, lot)\n",
    "\n",
    "    final_df = pd.concat([final_df,temp], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7d5ddaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\1184624018.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  club_totals['predicted_parking'] = 0\n"
     ]
    }
   ],
   "source": [
    "# concat club totals for onsite model next\n",
    "\n",
    "club_totals = total_future_table[total_future_table['location_group'] == 'Club']\n",
    "club_totals['predicted_parking'] = 0\n",
    "\n",
    "final_df = pd.concat([final_df, club_totals], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4cad384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure no negative predictions are made\n",
    "\n",
    "final_df['predicted_prepaid_additional_parking'] = np.where(final_df['predicted_parking'] < 0, 0, final_df['predicted_parking'])\n",
    "\n",
    "# get total prepaid tickets (current + predicted additional)\n",
    "\n",
    "final_df['total_predicted_prepaid_cars'] = final_df['prepaid_cars'] + final_df['predicted_parking']\n",
    "\n",
    "# get number of parked cars using historical show rates\n",
    "\n",
    "final_df = final_df.merge(paid_tiers, how =  'left', on = ['tier', 'location_group'])\n",
    "final_df['prepaid_cars_parked'] = (final_df['total_predicted_prepaid_cars'] * final_df['weighted_average']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d6360b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the capacity remaining \n",
    "\n",
    "final_df['cap_remaining'] = final_df['capacity'] - final_df['prepaid_cars_parked']\n",
    "\n",
    "# if predicted cars over capacity subtract overflow out\n",
    "\n",
    "final_df['predicted_prepaid_additional_parking'] = np.where(final_df['cap_remaining'] < 0, final_df['predicted_prepaid_additional_parking']+final_df['cap_remaining'], final_df['predicted_prepaid_additional_parking'])\n",
    "final_df['prepaid_cars_parked'] = np.where(final_df['cap_remaining'] < 0, final_df['prepaid_cars_parked']+final_df['cap_remaining'], final_df['prepaid_cars_parked'])\n",
    "final_df['cap_remaining'] = np.where(final_df['cap_remaining'] < 0, 0, final_df['prepaid_cars_parked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "af0a2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[['event_date','days_out','tier', 'start_time_num','weekend',\n",
    "                     'location_group','capacity','prepaid_cars','current_gross_revenue', \n",
    "                     'predicted_prepaid_additional_parking', 'total_predicted_prepaid_cars',\n",
    "                     'prepaid_cars_parked','cap_remaining']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d1c556e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get hisotrical onsite parking data\n",
    "\n",
    "q = \"\"\"\n",
    "with onsite as\n",
    "    (select\n",
    "        date(cth_game_descriptions.event_datetime) as event_date,\n",
    "        location_group,\n",
    "        0 as days_out,\n",
    "        case\n",
    "            when paid_amount > 0 then 1\n",
    "        else 0\n",
    "        end as num_onsite_cars,\n",
    "        case\n",
    "            when paid_amount = 0 then 1\n",
    "        else 0\n",
    "        end as num_prepaid_cars\n",
    "    from\n",
    "        custom.parkhub_v_transactions\n",
    "    left join\n",
    "        custom.cth_game_descriptions on parkhub_v_transactions.event_datetime = cth_game_descriptions.event_datetime\n",
    "    where\n",
    "        season in ('2023-24','2024-25'))\n",
    "select\n",
    "    onsite.event_date,\n",
    "    onsite.location_group,\n",
    "    'onsite' as parking_type,\n",
    "    days_out,\n",
    "    sum(num_onsite_cars) as num_cars,\n",
    "    capacity - sum(num_prepaid_cars) as cap_remaining\n",
    "from\n",
    "    onsite\n",
    "left join\n",
    "    custom.ctp_parking_capacities on onsite.location_group = ctp_parking_capacities.location_group\n",
    "where\n",
    "    days_out >= 0\n",
    "group by\n",
    "    onsite.event_date,\n",
    "    onsite.location_group,\n",
    "    parking_type,\n",
    "    days_out,\n",
    "    capacity\n",
    "\"\"\"\n",
    "\n",
    "historical_onsite_parking_info = FLA_Redshift(**rs_creds).query_warehouse(sql_string=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0d00413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge hisotrical prepaid parking data with hisorical game data for training model\n",
    "\n",
    "total_onsite_table = historical_onsite_parking_info.merge(all_game_info, how = 'left', on = 'event_date')\n",
    "total_onsite_table = total_onsite_table[total_onsite_table['tier'].isin(['R1','R2','R3','SC'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ce33fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a model to predict onsite cars\n",
    "\n",
    "def run_onsite_model(df, df_future, lot):\n",
    "\n",
    "    x_train_table = df[df['location_group'] == lot]\n",
    "\n",
    "    x_train = x_train_table[['cap_remaining','weekend','start_time_num']]\n",
    "    y_train = x_train_table[['num_cars']]\n",
    "\n",
    "    x_test_table = df_future[df_future['location_group'] == lot]\n",
    "\n",
    "    x_test = x_test_table[['cap_remaining','weekend','start_time_num']]\n",
    "\n",
    "    #scalar = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    #poly_features = scalar.fit_transform(x_train)\n",
    "    scalar = StandardScaler()\n",
    "    poly_features = scalar.fit_transform(x_train)\n",
    "\n",
    "    polynomial = LinearRegression().fit(poly_features, np.array(y_train).ravel())\n",
    "    #predicted_train = polynomial.predict(poly_features)\n",
    "\n",
    "    poly_features2 = scalar.fit_transform(x_test)\n",
    "\n",
    "    #predicted_test = polynomial.predict(poly_features2)\n",
    "\n",
    "    return polynomial.predict(poly_features2).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "58fc0429",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\769201683.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp['predicted_onsite_parking'] = run_onsite_model(total_onsite_table, final_df, lot)\n",
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\769201683.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp['predicted_onsite_parking'] = run_onsite_model(total_onsite_table, final_df, lot)\n",
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\769201683.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  temp['predicted_onsite_parking'] = run_onsite_model(total_onsite_table, final_df, lot)\n"
     ]
    }
   ],
   "source": [
    "# only predicting for general, garage, and club not valet or executive\n",
    "\n",
    "lots = ['General','Garage','Club']\n",
    "\n",
    "final_df_onsite = pd.DataFrame()\n",
    "\n",
    "for lot in lots:\n",
    "\n",
    "    temp = final_df[final_df['location_group'] == lot]\n",
    "\n",
    "    temp['predicted_onsite_parking'] = run_onsite_model(total_onsite_table, final_df, lot)\n",
    "\n",
    "    final_df_onsite = pd.concat([final_df_onsite,temp], ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eee0a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\3926685836.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec_and_valet['predicted_prepaid_additional_parking'] = 0\n",
      "C:\\Users\\riffere\\AppData\\Local\\Temp\\ipykernel_27320\\3926685836.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec_and_valet['total_predicted_prepaid_cars'] = exec_and_valet['prepaid_cars']\n"
     ]
    }
   ],
   "source": [
    "# add back executive and valet parking and match fields from final_df\n",
    "\n",
    "exec_and_valet = total_future_table[total_future_table['location_group'].isin(['Executive','Valet'])]\n",
    "\n",
    "exec_and_valet['predicted_prepaid_additional_parking'] = 0\n",
    "exec_and_valet['total_predicted_prepaid_cars'] = exec_and_valet['prepaid_cars']\n",
    "\n",
    "exec_and_valet = exec_and_valet.merge(paid_tiers, how =  'left', on = ['tier', 'location_group'])\n",
    "exec_and_valet['weighted_average'] = exec_and_valet['weighted_average'].fillna(1)\n",
    "\n",
    "exec_and_valet['prepaid_cars_parked'] = (exec_and_valet['total_predicted_prepaid_cars'] * exec_and_valet['weighted_average']).astype(int)\n",
    "exec_and_valet['predicted_onsite_parking'] = 0\n",
    "exec_and_valet['total_parking'] = exec_and_valet['prepaid_cars_parked'] \n",
    "\n",
    "exec_and_valet = exec_and_valet[['event_date', 'days_out','tier','location_group','capacity','prepaid_cars',\n",
    "                                       'current_gross_revenue', 'predicted_prepaid_additional_parking',\n",
    "                                       'total_predicted_prepaid_cars', 'prepaid_cars_parked',\n",
    "                                       'predicted_onsite_parking','total_parking']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e8b53053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if predicted total over capacity subtract overflow out\n",
    "\n",
    "final_df_onsite['predicted_onsite_parking'] = [pred_onsite if pred_onsite <= cap_remaining else cap_remaining for pred_onsite, cap_remaining in zip(final_df_onsite['predicted_onsite_parking'], final_df_onsite['cap_remaining'])]\n",
    "\n",
    "final_df_onsite['total_parking'] = final_df_onsite['prepaid_cars_parked'] + final_df_onsite['predicted_onsite_parking']\n",
    "\n",
    "final_parking_model = final_df_onsite[['event_date', 'days_out','tier','location_group','capacity','prepaid_cars',\n",
    "                                       'current_gross_revenue', 'predicted_prepaid_additional_parking','total_predicted_prepaid_cars',\n",
    "                                       'prepaid_cars_parked','predicted_onsite_parking','total_parking']]\n",
    "\n",
    "# merge with executive and valet parking info\n",
    "\n",
    "final_parking_model_df = pd.concat([final_parking_model, exec_and_valet], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5f37d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_parking_model_df = final_parking_model_df.merge(pricing_info, how = 'left', on = ['tier', 'location_group'])\n",
    "\n",
    "final_parking_model_df['predicted_prepaid_additional_gross_revenue'] = (final_parking_model_df['predicted_prepaid_additional_parking'] * final_parking_model_df['highest_price']).fillna(0)\n",
    "\n",
    "final_parking_model_df['predicted_onsite_parking_gross_revenue'] = (final_parking_model_df['predicted_prepaid_additional_parking'] * final_parking_model_df['highest_price']*1.25).fillna(0)\n",
    "\n",
    "final_parking_model_df['predicted_gross_revenue'] = final_parking_model_df['predicted_prepaid_additional_gross_revenue'] + final_parking_model_df['predicted_onsite_parking_gross_revenue'] + final_parking_model_df['current_gross_revenue']\n",
    "\n",
    "final_parking_model_df = final_parking_model_df[['event_date', 'days_out','tier','location_group','capacity','prepaid_cars',\n",
    "                                       'current_gross_revenue', 'predicted_prepaid_additional_parking', 'predicted_prepaid_additional_gross_revenue',\n",
    "                                       'total_predicted_prepaid_cars', 'prepaid_cars_parked', 'predicted_onsite_parking',\n",
    "                                       'predicted_onsite_parking_gross_revenue','total_parking', 'predicted_gross_revenue']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
