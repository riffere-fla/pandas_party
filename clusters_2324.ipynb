{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catnip.fla_redshift import FLA_Redshift\n",
    "from sqlalchemy import null\n",
    "from datetime import datetime\n",
    "\n",
    "from prefect.blocks.system import Secret\n",
    "from typing import Dict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import optimize\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_redshift_credentials() -> Dict:\n",
    "\n",
    "    cred_dict = {\n",
    "        \"dbname\": Secret.load(\"stellar-redshift-db-name\").get(),\n",
    "        \"host\": Secret.load(\"stellar-redshift-host\").get(),\n",
    "        \"port\": 5439,\n",
    "        \"user\": Secret.load(\"stellar-redshift-user-name\").get(),\n",
    "        \"password\": Secret.load(\"stellar-redshift-password\").get(),\n",
    "\n",
    "        \"aws_access_key_id\": Secret.load(\"fla-s3-aws-access-key-id-east-1\").get(),\n",
    "        \"aws_secret_access_key\": Secret.load(\"fla-s3-aws-secret-access-key-east-1\").get(),\n",
    "        \"bucket\": Secret.load(\"fla-s3-bucket-name-east-1\").get(),\n",
    "        \"subdirectory\": \"us-east-1\",\n",
    "\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    return cred_dict\n",
    "\n",
    "with ThreadPoolExecutor(1) as pool:\n",
    "    rs_creds = pool.submit(lambda: get_redshift_credentials()).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "        WITH nightly AS (\n",
    "            SELECT\n",
    "                date(event_date) as event_date,\n",
    "                count(*) AS nightly_tickets\n",
    "            FROM\n",
    "                custom.cth_historical_all_1718_2223\n",
    "            WHERE\n",
    "                season != '2020-21'\n",
    "                AND is_comp = FALSE\n",
    "                AND ticket_type IN ('Singles', 'Flex')\n",
    "            GROUP BY\n",
    "                event_date\n",
    "            UNION\n",
    "            SELECT\n",
    "                date(event_datetime) as event_date,\n",
    "                count(*) AS nightly_tickets\n",
    "            FROM\n",
    "                custom.cth_v_ticket_2324\n",
    "            WHERE\n",
    "                is_comp = FALSE\n",
    "                AND ticket_type IN ('Singles', 'Flex')\n",
    "            GROUP BY\n",
    "                event_date\n",
    "        ),\n",
    "        atp AS (\n",
    "            SELECT\n",
    "                date(event_date) as event_date,\n",
    "                sum(gross_revenue)/count(*) AS atp\n",
    "            FROM\n",
    "                custom.cth_historical_all_1718_2223\n",
    "            WHERE\n",
    "                season != '2020-21'\n",
    "            GROUP BY\n",
    "                event_date\n",
    "            UNION\n",
    "            SELECT\n",
    "                date(event_datetime) as event_date,\n",
    "                sum(gross_revenue)/count(*) AS atp\n",
    "            FROM\n",
    "                custom.cth_v_ticket_2324\n",
    "            GROUP BY\n",
    "                event_date\n",
    "        ),\n",
    "        attendance AS (\n",
    "            SELECT\n",
    "                date(event_date) as event_date,\n",
    "                sum(did_attend) AS attendance\n",
    "            FROM\n",
    "                custom.cth_historical_all_1718_2223\n",
    "            GROUP BY\n",
    "                event_date\n",
    "            UNION\n",
    "            SELECT\n",
    "                date(event_datetime) as event_date,\n",
    "                count(*) AS attendance\n",
    "            FROM\n",
    "                custom.cth_v_attendance_2324\n",
    "            GROUP BY\n",
    "                event_date\n",
    "        )\n",
    "        SELECT\n",
    "            n.event_date,\n",
    "            season,\n",
    "            n.nightly_tickets,\n",
    "            atp.atp,\n",
    "            att.attendance,\n",
    "            week_day,\n",
    "            trimester,\n",
    "            original_six_plus_extra,\n",
    "            is_dense\n",
    "        FROM\n",
    "            nightly n\n",
    "        LEFT JOIN\n",
    "            atp ON n.event_date = atp.event_date\n",
    "        LEFT JOIN\n",
    "            attendance att ON n.event_date = att.event_date\n",
    "        LEFT JOIN\n",
    "            custom.cth_game_descriptions ON date(n.event_date) = date(cth_game_descriptions.event_date)\n",
    "        WHERE\n",
    "            n.event_date < (GETDATE() - 1)\n",
    "        ORDER BY\n",
    "            n.event_date               \n",
    "    \"\"\"\n",
    "\n",
    "df = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clusters(X: np.array, n_clusters: int) -> np.array:\n",
    "\n",
    "    kmeans = KMeans(n_clusters, random_state = 1693)\n",
    "    kmeans.fit(X)\n",
    "    clusters = kmeans.labels_\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def create_clusters(redshift_creds: Dict, df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    ## season list\n",
    "    seasons = ['2017-18', '2018-19', '2019-20', '2021-22', '2022-23', '2023-24']\n",
    "\n",
    "    ## filter out covid & pre/post-season\n",
    "    df = df[df['season'].isin(['2017-18', '2018-19', '2019-20', '2021-22', '2022-23', '2023-24'])]\n",
    "    df = df[df['is_regular_season'] == 1]\n",
    "\n",
    "    ## add summary statistics\n",
    "    df = pd.merge(left = df, right = get_summary_statistics(redshift_creds), how = \"inner\", on = \"event_date\")\n",
    "\n",
    "    df_final = pd.DataFrame()\n",
    "\n",
    "    for i in seasons:\n",
    "\n",
    "        df_temp = df[df['season'] == i]\n",
    "        df_clust_temp = df_temp[[\n",
    "            'week_day', \n",
    "            'trimester', \n",
    "            'original_six_plus_extra', \n",
    "            'is_dense', \n",
    "            'nightly_tickets', \n",
    "            'atp', \n",
    "            'attendance'\n",
    "            ]]\n",
    "        \n",
    "        x = np.array(df_clust_temp)\n",
    "        x_standard = StandardScaler().fit_transform(x)\n",
    "\n",
    "        ## get clusters\n",
    "        df_temp['cluster_season'] = get_clusters(x_standard, 4)\n",
    "        df_final = pd.concat([df_final,df_temp])\n",
    "    \n",
    "    ## reset group numbers to match\n",
    "    df_final['agg'] = df_final.atp * df_final.nightly_tickets\n",
    "    df_agg_temp = df_final.groupby(by = ['season','cluster_season'])[['agg']].mean()\n",
    "    df_agg_temp = pd.DataFrame(df_agg_temp).reset_index()\n",
    "    df_agg_temp['rank'] = df_agg_temp.groupby(by = ['season'])[['agg']].rank('max')\n",
    "    \n",
    "    # now reset all the values\n",
    "    df_final = df_final.merge(right = df_agg_temp, how = 'left', on = ['season', 'cluster_season'])\n",
    "    df_final = df_final[['week_day', 'trimester', 'original_six_plus_extra', 'is_dense', 'nightly_tickets', 'atp', 'attendance', 'rank']].rename({'rank':'cluster_season'})\n",
    "\n",
    "    ## select cols \n",
    "    df_clust = df_final[[\n",
    "        'week_day', \n",
    "        'trimester', \n",
    "        'original_six_plus_extra', \n",
    "        'is_dense', \n",
    "        'nightly_tickets', \n",
    "        'atp', \n",
    "        'attendance'\n",
    "        ]]\n",
    "\n",
    "    ## scale\n",
    "    x = np.array(df_clust)\n",
    "    x_standard = StandardScaler().fit_transform(x)\n",
    "\n",
    "    ## get clusters\n",
    "    df_final['cluster'] = get_clusters(x_standard, 4)\n",
    "\n",
    "    ## select cols\n",
    "    df_final = df_final[['event_date', 'cluster', 'season_cluster']]\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "    WITH nightly AS (\n",
    "            SELECT\n",
    "                date(event_date) as event_date,\n",
    "                count(*) AS nightly_tickets\n",
    "            FROM\n",
    "                custom.cth_historical_all_1718_2223\n",
    "            WHERE\n",
    "                season != '2020-21'\n",
    "                AND is_comp = FALSE\n",
    "                AND ticket_type IN ('Singles', 'Flex')\n",
    "            GROUP BY\n",
    "                event_date\n",
    "            UNION\n",
    "            SELECT\n",
    "                date(event_datetime) as event_date,\n",
    "                count(*) AS nightly_tickets\n",
    "            FROM\n",
    "                custom.cth_v_ticket_2324\n",
    "            WHERE\n",
    "                is_comp = FALSE\n",
    "                AND ticket_type IN ('Singles', 'Flex')\n",
    "            GROUP BY\n",
    "                event_date),\n",
    "        atp AS (\n",
    "            SELECT\n",
    "                date(event_date) as event_date,\n",
    "                sum(gross_revenue)/count(*) AS atp\n",
    "            FROM\n",
    "                custom.cth_historical_all_1718_2223\n",
    "            WHERE\n",
    "                season != '2020-21'\n",
    "            GROUP BY\n",
    "                event_date\n",
    "            UNION\n",
    "            SELECT\n",
    "                date(event_datetime) as event_date,\n",
    "                sum(gross_revenue)/count(*) AS atp\n",
    "            FROM\n",
    "                custom.cth_v_ticket_2324\n",
    "            GROUP BY\n",
    "                event_date)\n",
    "        SELECT\n",
    "            n.event_date,\n",
    "            season,\n",
    "            n.nightly_tickets,\n",
    "            atp.atp,\n",
    "            n.nightly_tickets*atp.atp as agg,\n",
    "            cluster_season\n",
    "        FROM\n",
    "            nightly n\n",
    "        LEFT JOIN\n",
    "            atp ON n.event_date = atp.event_date\n",
    "        LEFT JOIN\n",
    "            custom.cth_game_descriptions ON date(n.event_date) = date(cth_game_descriptions.event_date)\n",
    "        WHERE\n",
    "            n.event_date < (GETDATE() - 1) and game_type = 1\n",
    "        ORDER BY\n",
    "            n.event_date\"\"\"\n",
    "df = FLA_Redshift(**rs_creds).query_warehouse(sql_string = q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_date</th>\n",
       "      <th>season</th>\n",
       "      <th>nightly_tickets</th>\n",
       "      <th>atp</th>\n",
       "      <th>agg</th>\n",
       "      <th>cluster_season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-10-07</td>\n",
       "      <td>2017-18</td>\n",
       "      <td>3676</td>\n",
       "      <td>44.109010</td>\n",
       "      <td>162144.719443</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-10-12</td>\n",
       "      <td>2017-18</td>\n",
       "      <td>868</td>\n",
       "      <td>46.890624</td>\n",
       "      <td>40701.061430</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-10-20</td>\n",
       "      <td>2017-18</td>\n",
       "      <td>3748</td>\n",
       "      <td>51.143397</td>\n",
       "      <td>191685.452064</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-10-26</td>\n",
       "      <td>2017-18</td>\n",
       "      <td>644</td>\n",
       "      <td>44.742957</td>\n",
       "      <td>28814.464480</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-10-28</td>\n",
       "      <td>2017-18</td>\n",
       "      <td>1949</td>\n",
       "      <td>47.110317</td>\n",
       "      <td>91818.008250</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>2024-03-30</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>2062</td>\n",
       "      <td>76.415893</td>\n",
       "      <td>157569.570785</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>2024-04-09</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>1714</td>\n",
       "      <td>58.754066</td>\n",
       "      <td>100704.468400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>2024-04-11</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>1852</td>\n",
       "      <td>47.321129</td>\n",
       "      <td>87638.731227</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>2126</td>\n",
       "      <td>73.059234</td>\n",
       "      <td>155323.930624</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>2024-04-16</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>1215</td>\n",
       "      <td>98.752226</td>\n",
       "      <td>119983.955012</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>239 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     event_date   season  nightly_tickets        atp            agg  \\\n",
       "0    2017-10-07  2017-18             3676  44.109010  162144.719443   \n",
       "1    2017-10-12  2017-18              868  46.890624   40701.061430   \n",
       "2    2017-10-20  2017-18             3748  51.143397  191685.452064   \n",
       "3    2017-10-26  2017-18              644  44.742957   28814.464480   \n",
       "4    2017-10-28  2017-18             1949  47.110317   91818.008250   \n",
       "..          ...      ...              ...        ...            ...   \n",
       "234  2024-03-30  2023-24             2062  76.415893  157569.570785   \n",
       "235  2024-04-09  2023-24             1714  58.754066  100704.468400   \n",
       "236  2024-04-11  2023-24             1852  47.321129   87638.731227   \n",
       "237  2024-04-13  2023-24             2126  73.059234  155323.930624   \n",
       "238  2024-04-16  2023-24             1215  98.752226  119983.955012   \n",
       "\n",
       "     cluster_season  \n",
       "0                 4  \n",
       "1                 1  \n",
       "2                 4  \n",
       "3                 1  \n",
       "4                 2  \n",
       "..              ...  \n",
       "234               3  \n",
       "235               3  \n",
       "236               2  \n",
       "237               2  \n",
       "238               4  \n",
       "\n",
       "[239 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.groupby(by = ['season','cluster_season'])[['agg']].mean()\n",
    "df1 = pd.DataFrame(df1).reset_index()\n",
    "df1['rank'] = df1.groupby(by = ['season'])[['agg']].rank('max')\n",
    "df.merge(right = df1, how = 'left', on = ['season', 'cluster_season'])\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
